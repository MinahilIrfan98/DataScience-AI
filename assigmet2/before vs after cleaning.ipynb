{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e183031-d165-4895-91be-c73f6e161f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BEFORE CLEANING =====\n",
      "Shape: (1600000, 6)\n",
      "Missing Values: {'target': 0, 'ids': 0, 'date': 0, 'flag': 0, 'user': 0, 'text': 0}\n",
      "Duplicates: 0\n",
      "\n",
      "===== AFTER CLEANING =====\n",
      "Shape: (1599982, 6)\n",
      "Missing Values: {'target': 0, 'ids': 0, 'date': 0, 'flag': 0, 'user': 0, 'text': 0}\n",
      "Duplicates: 0\n",
      "\n",
      "===== BEFORE vs AFTER CLEANING REPORT =====\n",
      "            Aspect                                                     Before Cleaning                                                      After Cleaning\n",
      "Shape (rows, cols)                                                        (1600000, 6)                                                        (1599982, 6)\n",
      "    Missing Values {'target': 0, 'ids': 0, 'date': 0, 'flag': 0, 'user': 0, 'text': 0} {'target': 0, 'ids': 0, 'date': 0, 'flag': 0, 'user': 0, 'text': 0}\n",
      "        Duplicates                                                                   0                                                                   0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# ========== STEP 1: Extract Dataset ==========\n",
    "# Path to your zip file (change if needed)\n",
    "zip_path = r\"C:\\Users\\DELL\\Downloads\\archive (1).zip\"\n",
    "extract_path = r\"C:\\Users\\DELL\\Downloads\\sentiment_data\"\n",
    "\n",
    "# Extract the dataset\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = os.path.join(extract_path, 'training.1600000.processed.noemoticon.csv')\n",
    "df = pd.read_csv(file_path, encoding='latin-1', header=None)\n",
    "\n",
    "# Add column names (as per dataset description on Kaggle)\n",
    "df.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# ========== STEP 2: BEFORE CLEANING INFO ==========\n",
    "df_info_before = {\n",
    "    \"Shape\": df.shape,\n",
    "    \"Missing Values\": df.isnull().sum().to_dict(),\n",
    "    \"Duplicates\": df.duplicated().sum()\n",
    "}\n",
    "\n",
    "print(\"===== BEFORE CLEANING =====\")\n",
    "print(\"Shape:\", df_info_before[\"Shape\"])\n",
    "print(\"Missing Values:\", df_info_before[\"Missing Values\"])\n",
    "print(\"Duplicates:\", df_info_before[\"Duplicates\"])\n",
    "\n",
    "\n",
    "# ========== STEP 3: DATA CLEANING ==========\n",
    "\n",
    "# 1. Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# 2. Handle missing values\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# 3. Treat outliers in 'text' column\n",
    "# Keep only tweets with length between 3 and 280 characters\n",
    "df_cleaned = df_cleaned[df_cleaned['text'].str.len().between(3, 280)]\n",
    "\n",
    "\n",
    "# ========== STEP 4: AFTER CLEANING INFO ==========\n",
    "df_info_after = {\n",
    "    \"Shape\": df_cleaned.shape,\n",
    "    \"Missing Values\": df_cleaned.isnull().sum().to_dict(),\n",
    "    \"Duplicates\": df_cleaned.duplicated().sum()\n",
    "}\n",
    "\n",
    "print(\"\\n===== AFTER CLEANING =====\")\n",
    "print(\"Shape:\", df_info_after[\"Shape\"])\n",
    "print(\"Missing Values:\", df_info_after[\"Missing Values\"])\n",
    "print(\"Duplicates:\", df_info_after[\"Duplicates\"])\n",
    "\n",
    "\n",
    "# ========== STEP 5: BEFORE vs AFTER REPORT ==========\n",
    "report_data = {\n",
    "    \"Aspect\": [\"Shape (rows, cols)\", \"Missing Values\", \"Duplicates\"],\n",
    "    \"Before Cleaning\": [\n",
    "        str(df_info_before[\"Shape\"]),\n",
    "        str(df_info_before[\"Missing Values\"]),\n",
    "        df_info_before[\"Duplicates\"]\n",
    "    ],\n",
    "    \"After Cleaning\": [\n",
    "        str(df_info_after[\"Shape\"]),\n",
    "        str(df_info_after[\"Missing Values\"]),\n",
    "        df_info_after[\"Duplicates\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "report_df = pd.DataFrame(report_data)\n",
    "print(\"\\n===== BEFORE vs AFTER CLEANING REPORT =====\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# Optionally save the cleaned dataset\n",
    "df_cleaned.to_csv(r\"C:\\Users\\DELL\\Downloads\\twitter_sentiment_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacdeac-97cd-4afb-9d6e-6d0f83590aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ac3c8-337f-45f0-b68e-c856b8ca7804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3408a9-e3c9-4208-a52d-5867736340ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02ebba-632a-470e-b546-78d91fa36bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
